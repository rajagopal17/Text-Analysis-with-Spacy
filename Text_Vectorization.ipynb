{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Vectorization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRVML1A/IWwCGQBYbGnw+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajagopal17/Text-Analysis-with-Spacy/blob/master/Text_Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85aqZGYapl80",
        "colab_type": "text"
      },
      "source": [
        "##https://www.kaggle.com/zackakil/done-nlp-using-word-vectors-with-spacy-cldspn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wspQq5VHxKd",
        "colab_type": "text"
      },
      "source": [
        "##https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfdI2MwlHfbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "174b0ee8-1864-41c9-9130-7e2721991681"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rc('font', size=14)\n",
        "sns.set(style='white')\n",
        "sns.set(style='whitegrid', color_codes=True)\n",
        "import csv\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0WEl7RMHm70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "340fc235-5dd1-4d86-ff07-e0f6e9fa6fb5"
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "import spacy\n",
        "from spacy.tokens import doc\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span\n",
        "from spacy import displacy\n",
        "print (spacy.__version__)\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp=spacy.load('en_core_web_lg')\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "import string\n",
        "punctuations=string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtDoablfRSK0",
        "colab_type": "text"
      },
      "source": [
        "##Download Large model for Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDronqH-Pk-H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "874873fb-9e23-4bac-f5ba-9c51f0767c7b"
      },
      "source": [
        "#!python -m spacy download en_core_web_lg\n",
        "################################################\n",
        "#import spacy.cli\n",
        "#spacy.cli.download(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikzgxc7LJ_JU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4249fefe-b8e5-4e06-e665-5b6e5a3a7927"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize(text):\n",
        "    stem = nltk.stem.SnowballStemmer('english')\n",
        "    text = text.lower()\n",
        "\n",
        "    for token in nltk.word_tokenize(text):\n",
        "        if token in string.punctuation: continue\n",
        "        yield stem.stem(token)\n",
        "\n",
        "corpus = [\n",
        "    \"The elephant sneezed at the sight of potatoes.\",\n",
        "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
        "    \"Wondering, she opened the door to the studio.\",\n",
        "]\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def vectorize(doc):\n",
        "    features = defaultdict(int)\n",
        "    for token in tokenize(doc):\n",
        "        features[token] += 1\n",
        "        print(features)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuWo-dImKmb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = map(vectorize, corpus)\n",
        "list(vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt94gufDMJ87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "320a7e05-f1b6-4c02-b469-6cb575b3da60"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit(corpus)\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 16, 'elephant': 6, 'sneezed': 14, 'at': 0, 'sight': 12, 'of': 7, 'potatoes': 9, 'bats': 2, 'can': 3, 'see': 10, 'via': 18, 'echolocation': 5, 'bat': 1, 'sneeze': 13, 'wondering': 19, 'she': 11, 'opened': 8, 'door': 4, 'to': 17, 'studio': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsK9E520OqFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "504b469a-30db-4977-d867-e4b48d98e37f"
      },
      "source": [
        "# If we would like to actually create a vector, we can do so by passing the\n",
        "# text into the vectorizer to get back counts\n",
        "vector = vectorizer.transform(corpus)\n",
        "print(vector.toarray())\n",
        "                              "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 2 0 0 0]\n",
            " [0 1 1 1 0 1 0 0 0 0 2 0 1 1 0 0 1 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 2 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsIA_pEdQPBy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a81379c1-2622-42f5-edfb-2605bf285353"
      },
      "source": [
        "vectorizer.transform(['elephant']).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTh2qmpXSpuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "88888ed1-ce20-48e8-9e9f-e342bb5359fa"
      },
      "source": [
        "print(vectorizer.fit_transform(corpus).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 2 0 0 0]\n",
            " [0 1 1 1 0 1 0 0 0 0 2 0 1 1 0 0 1 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 2 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2qGXHepTK51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2672e488-3ff5-47f9-c824-9eca5556f724"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 16, 'elephant': 6, 'sneezed': 14, 'at': 0, 'sight': 12, 'of': 7, 'potatoes': 9, 'bats': 2, 'can': 3, 'see': 10, 'via': 18, 'echolocation': 5, 'bat': 1, 'sneeze': 13, 'wondering': 19, 'she': 11, 'opened': 8, 'door': 4, 'to': 17, 'studio': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjXARz2GWikM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "doc=nlp(str(corpus))\n",
        "for token in doc:\n",
        "  print(token)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqs0VfSxIke3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d65993c-e40b-4b60-c696-03aa245cb1ba"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "nlp('doctor').similarity(nlp('medical'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7008181125452967"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urR7iuIJHJBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp('elephant').vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsAV4DeUWMHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in doc:\n",
        "    print(word, nlp('hear').similarity(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-kQhGhHa4Oo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59ad0cad-92b7-41e7-ee3a-296a0b7c021d"
      },
      "source": [
        "nlp('what ever you want to say').vector.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.024536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}